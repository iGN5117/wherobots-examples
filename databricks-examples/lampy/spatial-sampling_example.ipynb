{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58006583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Import GeoPandas\n",
    "import geopandas as gpd\n",
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, FloatType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Import Apache Sedona\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.utils.adapter import Adapter as adp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3913aba4",
   "metadata": {},
   "source": [
    "## Define spark session if not defined yet\n",
    "No need to define spark if run in an external cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69163745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/02 12:16:41 WARN Utils: Your hostname, Kanchans-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.1.13 instead (on interface en0)\n",
      "23/03/02 12:16:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/kanchan/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kanchan/.ivy2/jars\n",
      "org.apache.sedona#sedona-python-adapter-3.0_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8c19fe1e-55aa-4a5e-accb-78ec6c608f24;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-python-adapter-3.0_2.12;1.3.1-incubating in central\n",
      "\tfound org.locationtech.jts#jts-core;1.18.2 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.apache.sedona#sedona-core-3.0_2.12;1.3.1-incubating in central\n",
      "\tfound org.apache.sedona#sedona-common;1.3.1-incubating in central\n",
      "\tfound org.apache.sedona#sedona-sql-3.0_2.12;1.3.1-incubating in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.1.0-25.2 in central\n",
      ":: resolution report :: resolve 141ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.sedona#sedona-common;1.3.1-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-core-3.0_2.12;1.3.1-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-python-adapter-3.0_2.12;1.3.1-incubating from central in [default]\n",
      "\torg.apache.sedona#sedona-sql-3.0_2.12;1.3.1-incubating from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.1.0-25.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.18.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8c19fe1e-55aa-4a5e-accb-78ec6c608f24\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/02 12:16:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('appName'). \\\n",
    "    master('local[*]'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "    config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c7a1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee4b6c",
   "metadata": {},
   "source": [
    "# Use the prefix in all your DBFS path\n",
    "\n",
    "If you use DBFS, Databricks requires that all paths must be absolute. You can use the variable below as the prefix for all paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf9777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH_PREFIX= str(Path.home()) + '/' if os.environ.get('ENV_WB', 'false') == 'true' else '/'\n",
    "\n",
    "print(PATH_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb42eb6",
   "metadata": {},
   "source": [
    "## Load Taxi Pick Up Data to Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570346c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------------------+---------------+-------------------+-------------------+------------------+---------+-----------------+-------------------+------------------+------------+------------------+---------+-------+------------------+---------+------------------+\n",
      "|vendor_name|Trip_Pickup_DateTime|Trip_Dropoff_DateTime|Passenger_Count|      Trip_Distance|          Start_Lon|         Start_Lat|Rate_Code|store_and_forward|            End_Lon|           End_Lat|Payment_Type|          Fare_Amt|surcharge|mta_tax|           Tip_Amt|Tolls_Amt|         Total_Amt|\n",
      "+-----------+--------------------+---------------------+---------------+-------------------+-------------------+------------------+---------+-----------------+-------------------+------------------+------------+------------------+---------+-------+------------------+---------+------------------+\n",
      "|        VTS| 2009-01-04 02:52:00|  2009-01-04 03:02:00|              1| 2.6299999999999999|-73.991956999999999|         40.721567|     null|             null|         -73.993803|40.695922000000003|        CASH|8.9000000000000004|      0.5|   null|                 0|        0|9.4000000000000004|\n",
      "|        VTS| 2009-01-04 03:31:00|  2009-01-04 03:38:00|              3| 4.5499999999999998|-73.982101999999998|40.736289999999997|     null|             null|-73.955849999999998|40.768030000000003|      Credit|              12.1|      0.5|   null|                 2|        0|              14.6|\n",
      "|        VTS| 2009-01-03 15:43:00|  2009-01-03 15:57:00|              5|              10.35|-74.002587000000005|40.739747999999999|     null|             null|-73.869983000000005|40.770225000000003|      Credit|23.699999999999999|        0|   null|4.7400000000000002|        0|28.440000000000001|\n",
      "|        DDS| 2009-01-01 20:52:58|  2009-01-01 21:14:00|              1|                  5|-73.974266999999998|40.790954999999997|     null|             null|-73.996557999999993|40.731848999999997|      CREDIT|              14.9|      0.5|   null|3.0499999999999998|        0|18.449999999999999|\n",
      "|        DDS| 2009-01-24 16:18:23|  2009-01-24 16:24:56|              1|0.40000000000000002|-74.001580000000004|40.719382000000003|     null|             null|-74.008377999999993|40.720350000000003|        CASH|3.7000000000000002|        0|   null|                 0|        0|3.7000000000000002|\n",
      "+-----------+--------------------+---------------------+---------------+-------------------+-------------------+------------------+---------+-----------------+-------------------+------------------+------------+------------------+---------+-------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiDf = spark.read.format(\"csv\").option(\"header\", True).load(\"s3a://wherobots-examples/data/nyc-taxi-data.csv\")\n",
    "taxiDf = taxiDf.filter(taxiDf.Start_Lat >= 40).filter(taxiDf.Start_Lat <= 41).filter(taxiDf.Start_Lon >= -75).filter(taxiDf.Start_Lon <= -73)\n",
    "taxiDf.createOrReplaceTempView(\"taxiDf\")\n",
    "taxiDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15063c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          pickup_loc|        pickup_time|\n",
      "+--------------------+-------------------+\n",
      "|POINT (-73.991957...|2009-01-04 02:52:00|\n",
      "|POINT (-73.982102...|2009-01-04 03:31:00|\n",
      "|POINT (-74.002587...|2009-01-03 15:43:00|\n",
      "|POINT (-73.974267...|2009-01-01 20:52:58|\n",
      "|POINT (-74.00158 ...|2009-01-24 16:18:23|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Select only required or target columns\n",
    "taxiDf = spark.sql(\"select ST_Point(Double(Start_Lon), Double(Start_Lat)) as pickup_loc, Trip_Pickup_DateTime as pickup_time from taxiDf\")\n",
    "taxiDf.createOrReplaceTempView(\"taxiDf\")\n",
    "taxiDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cd2df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13876530"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4bf7e",
   "metadata": {},
   "source": [
    "## Spatial Sampling Analysis\n",
    "Perform spatially stratified sampling with lampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d28d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lampy import SpatialSampling as ss\n",
    "from lampy import SparkRegistration\n",
    "\n",
    "SparkRegistration.set_spark_session(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e23869e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/02 12:16:54 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===================================>                    (12 + 7) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          pickup_loc|        pickup_time|\n",
      "+--------------------+-------------------+\n",
      "|POINT (-75.968462...|2009-01-24 00:46:00|\n",
      "|POINT (-75.95602 ...|2009-01-24 00:18:00|\n",
      "|POINT (-75.764997...|2009-01-23 19:05:00|\n",
      "|POINT (-75.806533...|2009-01-26 22:58:00|\n",
      "|POINT (-75.571363...|2009-01-26 05:52:00|\n",
      "|POINT (-74.866317...|2009-01-16 19:24:00|\n",
      "|POINT (-74.890647...|2009-01-16 20:10:00|\n",
      "|POINT (-74.874623...|2009-01-16 18:58:00|\n",
      "|POINT (-75.844613...|2009-01-27 00:41:00|\n",
      "|POINT (-74.831133...|2009-01-07 06:28:00|\n",
      "+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:=====================================================>  (18 + 1) / 19]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ssDf = ss.get_spatially_stratified_samples(taxiDf, \"pickup_loc\", 0.1)\n",
    "ssDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3939d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/02 12:17:01 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1389530"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7408a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
