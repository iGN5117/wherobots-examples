{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58006583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Import GeoPandas\n",
    "import geopandas as gpd\n",
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, FloatType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Import Apache Sedona\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.utils.adapter import Adapter as adp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3913aba4",
   "metadata": {},
   "source": [
    "## Define spark session if not defined yet\n",
    "No need to define spark if run in an external cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69163745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 13:08:32 WARN Utils: Your hostname, Kanchans-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.1.13 instead (on interface en0)\n",
      "23/03/13 13:08:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 13:08:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('appName'). \\\n",
    "    master('local[*]'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "    config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c7a1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee4b6c",
   "metadata": {},
   "source": [
    "# Use the prefix in all your DBFS path\n",
    "\n",
    "If you use DBFS, Databricks requires that all paths must be absolute. You can use the variable below as the prefix for all paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf9777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH_PREFIX= str(Path.home()) + '/' if os.environ.get('ENV_WB', 'false') == 'true' else '/'\n",
    "\n",
    "print(PATH_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb42eb6",
   "metadata": {},
   "source": [
    "## Load Taxi Pick Up Data to Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570346c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------------------+---------------+-------------------+-------------------+------------------+---------+-----------------+-------------------+------------------+------------+------------------+---------+-------+------------------+---------+------------------+\n",
      "|vendor_name|Trip_Pickup_DateTime|Trip_Dropoff_DateTime|Passenger_Count|      Trip_Distance|          Start_Lon|         Start_Lat|Rate_Code|store_and_forward|            End_Lon|           End_Lat|Payment_Type|          Fare_Amt|surcharge|mta_tax|           Tip_Amt|Tolls_Amt|         Total_Amt|\n",
      "+-----------+--------------------+---------------------+---------------+-------------------+-------------------+------------------+---------+-----------------+-------------------+------------------+------------+------------------+---------+-------+------------------+---------+------------------+\n",
      "|        VTS| 2009-01-04 02:52:00|  2009-01-04 03:02:00|              1| 2.6299999999999999|-73.991956999999999|         40.721567|     null|             null|         -73.993803|40.695922000000003|        CASH|8.9000000000000004|      0.5|   null|                 0|        0|9.4000000000000004|\n",
      "|        VTS| 2009-01-04 03:31:00|  2009-01-04 03:38:00|              3| 4.5499999999999998|-73.982101999999998|40.736289999999997|     null|             null|-73.955849999999998|40.768030000000003|      Credit|              12.1|      0.5|   null|                 2|        0|              14.6|\n",
      "|        VTS| 2009-01-03 15:43:00|  2009-01-03 15:57:00|              5|              10.35|-74.002587000000005|40.739747999999999|     null|             null|-73.869983000000005|40.770225000000003|      Credit|23.699999999999999|        0|   null|4.7400000000000002|        0|28.440000000000001|\n",
      "|        DDS| 2009-01-01 20:52:58|  2009-01-01 21:14:00|              1|                  5|-73.974266999999998|40.790954999999997|     null|             null|-73.996557999999993|40.731848999999997|      CREDIT|              14.9|      0.5|   null|3.0499999999999998|        0|18.449999999999999|\n",
      "|        DDS| 2009-01-24 16:18:23|  2009-01-24 16:24:56|              1|0.40000000000000002|-74.001580000000004|40.719382000000003|     null|             null|-74.008377999999993|40.720350000000003|        CASH|3.7000000000000002|        0|   null|                 0|        0|3.7000000000000002|\n",
      "+-----------+--------------------+---------------------+---------------+-------------------+-------------------+------------------+---------+-----------------+-------------------+------------------+------------+------------------+---------+-------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxiDf = spark.read.format(\"csv\").option(\"header\", True).load(\"s3a://wherobots-examples/data/nyc-taxi-data.csv\")\n",
    "taxiDf = taxiDf.filter(taxiDf.Start_Lat >= 40).filter(taxiDf.Start_Lat <= 41).filter(taxiDf.Start_Lon >= -75).filter(taxiDf.Start_Lon <= -73)\n",
    "taxiDf.createOrReplaceTempView(\"taxiDf\")\n",
    "taxiDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15063c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|          pickup_loc|        pickup_time|\n",
      "+--------------------+-------------------+\n",
      "|POINT (-73.991957...|2009-01-04 02:52:00|\n",
      "|POINT (-73.982102...|2009-01-04 03:31:00|\n",
      "|POINT (-74.002587...|2009-01-03 15:43:00|\n",
      "|POINT (-73.974267...|2009-01-01 20:52:58|\n",
      "|POINT (-74.00158 ...|2009-01-24 16:18:23|\n",
      "+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Remove unnecessary columns and convert Start_lat and Start_Lon into Geometry type Points\n",
    "taxiDf = spark.sql(\"select ST_Point(Double(Start_Lon), Double(Start_Lat)) as pickup_loc, Trip_Pickup_DateTime as pickup_time from taxiDf\")\n",
    "taxiDf.createOrReplaceTempView(\"taxiDf\")\n",
    "taxiDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4bf7e",
   "metadata": {},
   "source": [
    "## Hotspot Analysis\n",
    "Hotspot analysis finds out those places and time periods which have the highest occurrence of a target event. Lampy allows hotspot analysis in two dimensions: spatial and temporal. In spatial dimension, it converts the spatial coverage of the dataset into a grid. Usage of temporal dimension is optional, it will be considered only if parameter col_date is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d28d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lampy import HotspotAnalysis as ha\n",
    "from lampy import SparkRegistration\n",
    "\n",
    "SparkRegistration.set_spark_session(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e23869e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:======================>                                  (4 + 6) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------+\n",
      "|       cell_geometry|_id_timestep|hotspot_count|\n",
      "+--------------------+------------+-------------+\n",
      "|POLYGON ((-73.826...|          91|            1|\n",
      "|POLYGON ((-73.826...|         163|            1|\n",
      "|POLYGON ((-73.826...|         646|            1|\n",
      "|POLYGON ((-73.826...|         665|            1|\n",
      "|POLYGON ((-73.826...|          33|            1|\n",
      "|POLYGON ((-73.826...|          42|            1|\n",
      "|POLYGON ((-73.826...|          45|            1|\n",
      "|POLYGON ((-73.826...|          64|            1|\n",
      "|POLYGON ((-73.826...|          82|            1|\n",
      "|POLYGON ((-73.826...|          87|            1|\n",
      "|POLYGON ((-73.826...|          88|            2|\n",
      "|POLYGON ((-73.826...|         108|            1|\n",
      "|POLYGON ((-73.826...|         109|            1|\n",
      "|POLYGON ((-73.826...|         118|            1|\n",
      "|POLYGON ((-73.826...|         130|            1|\n",
      "|POLYGON ((-73.826...|         138|            1|\n",
      "|POLYGON ((-73.826...|         151|            1|\n",
      "|POLYGON ((-73.826...|         168|            1|\n",
      "|POLYGON ((-73.826...|         175|            1|\n",
      "|POLYGON ((-73.826...|         177|            1|\n",
      "+--------------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "haDf = ha.get_hotspots(taxiDf, \"pickup_loc\", 10, 10)\n",
    "haDf.createOrReplaceTempView(\"haDf\")\n",
    "haDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d60b4a",
   "metadata": {},
   "source": [
    "### Hotspot Analysis with Temporal Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6efd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "haTimeDf = ha.get_hotspots(taxiDf, \"pickup_loc\", 10, 10, col_date=\"pickup_time\", date_format=\"M/d/yy H:m\")\n",
    "haTimeDf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
