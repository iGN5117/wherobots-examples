{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"false\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58006583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Import GeoPandas\n",
    "import geopandas as gpd\n",
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, FloatType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Import Apache Sedona\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.utils.adapter import Adapter as adp\n",
    "from sedona.core.enums import GridType, IndexType\n",
    "from sedona.core.SpatialRDD import CircleRDD\n",
    "from sedona.core.spatialOperator import JoinQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3913aba4",
   "metadata": {},
   "source": [
    "## Define spark session if not defined yet\n",
    "No need to define spark if run in an external cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69163745",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('appName'). \\\n",
    "    master('local[*]'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "    config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a1aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "\n",
    "def delete_path(sc, path):\n",
    "    fs = (sc._jvm.org\n",
    "          .apache.hadoop\n",
    "          .fs.FileSystem\n",
    "          .get(sc._jsc.hadoopConfiguration())\n",
    "          )\n",
    "    fs.delete(sc._jvm.org.apache.hadoop.fs.Path(path), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee4b6c",
   "metadata": {},
   "source": [
    "## Use the prefix in all your EMR path\n",
    "\n",
    "If you use EMR, EMR requires that all paths must be relative. Please use the variable below as the prefix for all paths because it can automatically detect if you are in Wherobots environment or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf9777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH_PREFIX= str(Path.home()) + '/' if os.environ.get('ENV_WB', 'false') == 'true' else ''\n",
    "\n",
    "print(PATH_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898a85a",
   "metadata": {},
   "source": [
    "## Load Area Landmark Data to Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d8e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_rdd = ShapefileReader.readToGeometryRDD(sc, \"s3a://wherobots-examples/data/nyc-area-landmark-shapefile\")\n",
    "area_df = adp.toDf(area_rdd, spark)\n",
    "area_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44dc15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- statefp: string (nullable = true)\n",
      " |-- ansicode: string (nullable = true)\n",
      " |-- areaid: string (nullable = true)\n",
      " |-- fullname: string (nullable = true)\n",
      " |-- mtfcc: string (nullable = true)\n",
      " |-- aland: string (nullable = true)\n",
      " |-- awater: string (nullable = true)\n",
      " |-- intptlat: string (nullable = true)\n",
      " |-- intptlon: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "area_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9048b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            geometry|\n",
      "+--------------------+\n",
      "|POLYGON ((-73.894...|\n",
      "|POLYGON ((-73.781...|\n",
      "|POLYGON ((-73.911...|\n",
      "|POLYGON ((-73.772...|\n",
      "|POLYGON ((-73.787...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "area_df = area_df.select(\"geometry\")\n",
    "area_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9fb0a",
   "metadata": {},
   "source": [
    "## Load Taxi Trip Data to Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36dc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = spark.read.format(\"csv\").option(\"header\", True).load(\"s3a://wherobots-examples/data/nyc-taxi-data.csv\")\n",
    "taxi_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87f8ea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               point|\n",
      "+--------------------+\n",
      "|POINT (-73.991957...|\n",
      "|POINT (-73.982102...|\n",
      "|POINT (-74.002587...|\n",
      "|POINT (-73.974267...|\n",
      "|POINT (-74.00158 ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_df.createOrReplaceTempView(\"taxi_df\")\n",
    "taxi_df = spark.sql(\"select ST_Point(Double(Start_Lon), Double(Start_Lat)) as point from taxi_df\")\n",
    "taxi_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034d2fd",
   "metadata": {},
   "source": [
    "## Colocation Pattern Detection\n",
    "There are various algorithms for detecting colocation pattern between two spatial datasets. Here, we use the algorithm based on Ripley's K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d36fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lampy import ColocationPatterns, SparkRegistration\n",
    "\n",
    "SparkRegistration.set_spark_session(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a414ece3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colocated\n",
      "Required time: 3.1826131343841553\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "colocated = ColocationPatterns.get_colocation_pattern_on_ripleys_k(area_df, taxi_df, \"geometry\", \"point\")\n",
    "if colocated:\n",
    "    print(\"Colocated\")\n",
    "else:\n",
    "    print(\"Not Colocated\")\n",
    "t2 = time.time()\n",
    "print(\"Required time:\", t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7408a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
