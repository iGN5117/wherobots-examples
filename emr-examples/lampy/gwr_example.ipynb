{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"false\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3a56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Import GeoPandas\n",
    "import geopandas as gpd\n",
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, FloatType\n",
    "from pyspark.sql.functions import col, array, udf\n",
    "\n",
    "# Import Apache Sedona\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.utils.adapter import Adapter as adp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7646a",
   "metadata": {},
   "source": [
    "## Define spark session if not defined yet\n",
    "No need to define spark if run in an external cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16320942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 10:05:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('appName'). \\\n",
    "    master('local[*]'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "    config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f463e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "\n",
    "def delete_path(sc, path):\n",
    "    fs = (sc._jvm.org\n",
    "          .apache.hadoop\n",
    "          .fs.FileSystem\n",
    "          .get(sc._jsc.hadoopConfiguration())\n",
    "          )\n",
    "    fs.delete(sc._jvm.org.apache.hadoop.fs.Path(path), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67640d",
   "metadata": {},
   "source": [
    "## Use the prefix in all your EMR path\n",
    "\n",
    "If you use EMR, EMR requires that all paths must be relative. Please use the variable below as the prefix for all paths because it can automatically detect if you are in Wherobots environment or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0439502e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wherobots/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH_PREFIX= str(Path.home()) + '/' if os.environ.get('ENV_WB', 'false') == 'true' else ''\n",
    "\n",
    "print(PATH_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f62dc",
   "metadata": {},
   "source": [
    "## Load Airbnb Data to Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e7aa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 10:05:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------+----+------------+------------------+------------------+\n",
      "|         log_price|host_listings_count|bedrooms|beds|accommodates|          latitude|         longitude|\n",
      "+------------------+-------------------+--------+----+------------+------------------+------------------+\n",
      "| 4.941642429752162|                1.0|     1.0| 2.0|         3.0|          30.26057|         -97.73441|\n",
      "|3.7135720910945516|                1.0|     1.0| 1.0|         2.0|30.456970000000002|-97.78421999999999|\n",
      "| 5.267858164217968|                1.0|     2.0| 4.0|         4.0|          30.80862|           -98.374|\n",
      "| 6.542471961947727|                1.0|     2.0| 3.0|         5.0|          30.25328|         -97.72968|\n",
      "| 5.257495377236115|                3.0|     3.0| 3.0|         8.0|30.283540000000002|         -97.64966|\n",
      "+------------------+-------------------+--------+----+------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "listings_df = spark.read.format(\"csv\").option(\"header\",True).load(\"s3a://wherobots-examples/data/airbnb_listings.csv\")\n",
    "listings_df = listings_df.drop(\"price\")\n",
    "listings_df.createOrReplaceTempView(\"listings_df\")\n",
    "\n",
    "listings_df = spark.sql(\"select double(log_price), double(host_listings_count), double(bedrooms), double(beds), double(accommodates), double(latitude) as latitude, double(longitude) as longitude from listings_df\")\n",
    "listings_df.createOrReplaceTempView(\"listings_df\")\n",
    "listings_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2f306",
   "metadata": {},
   "source": [
    "## Split the DataFrame into Train and Test Parts (60/40 Percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "063befa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = listings_df.randomSplit([0.6, 0.4], 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b63d0",
   "metadata": {},
   "source": [
    "## Perform Geographically Weighted Regression\n",
    "Cluster the points based on DBSCAN algorithm. For each point the returned DataFrame conatins the cluster labels with the column name component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd92e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lampy import ML_GWR\n",
    "from lampy import SparkRegistration\n",
    "\n",
    "SparkRegistration.set_spark_session(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33bcc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names = ['host_listings_count', 'bedrooms', 'beds', 'accommodates']\n",
    "y_name = 'log_price'\n",
    "lat_name = 'latitude'\n",
    "lon_name = 'longitude'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06f88f",
   "metadata": {},
   "source": [
    "### Train GWR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ML_GWR()\n",
    "train_results = model.fit(train_df, lat_name, lon_name, x_names, y_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e01341",
   "metadata": {},
   "source": [
    "### Test the Trained Model with Test DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = model.predict(test_df, lat_name, lon_name, x_names)\n",
    "test_predictions = pred_results.predictions\n",
    "\n",
    "y_test = np.array(list(test_df.select(y_name).toPandas()[y_name])).T\n",
    "mae = abs(y_test - test_predictions)\n",
    "print('Mean Absolute Error:', round(np.mean(mae), 2))\n",
    "\n",
    "mse = ((y_test - test_predictions)**2).mean()\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root mean square error: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059c3fb",
   "metadata": {},
   "source": [
    "### Get the Model Parameters as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_df = model.get_model_params() # You can write the model_params_df dataframe to any path for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d65e1e",
   "metadata": {},
   "source": [
    "### Load a New Model with the Previuosly Saved Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = ML_GWR()\n",
    "model_new.load_model_params(model_params_df) # First, you should read model_params_df from the path where it was stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c0169",
   "metadata": {},
   "source": [
    "### Test the New Loaded Model with Test DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = model_new.predict(test_df, lat_name, lon_name, x_names)\n",
    "test_predictions = pred_results.predictions\n",
    "\n",
    "y_test = np.array(list(test_df.select(y_name).toPandas()[y_name])).T\n",
    "mae = abs(y_test - test_predictions)\n",
    "print('Mean Absolute Error:', round(np.mean(mae), 2))\n",
    "\n",
    "mse = ((y_test - test_predictions)**2).mean()\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root mean square error: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673fc03",
   "metadata": {},
   "source": [
    "## Perfrom Post Processing on Output and Test Data For Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e10990",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.squeeze(test_predictions).tolist()\n",
    "def get_pred_column_value(id):\n",
    "    return test_predictions[id]\n",
    "udf_get_prediction = udf(get_pred_column_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.rdd.zipWithIndex().toDF().select(col(\"_1.*\"), col(\"_2\").alias('idx'))\n",
    "test_df = test_df.withColumn(\"predicted_log_price\", udf_get_prediction(test_df.idx))\n",
    "test_df.createOrReplaceTempView(\"test_df\")\n",
    "\n",
    "test_df = spark.sql(\"select idx, log_price, predicted_log_price, ST_Point(double(longitude), double(latitude)) as airbnb_loc from test_df\")\n",
    "test_df.createOrReplaceTempView(\"test_df\")\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed90683",
   "metadata": {},
   "source": [
    "### Loading Shape File for Austin Zip Code Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b520cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "austin_rdd = ShapefileReader.readToGeometryRDD(sc, \"s3a://wherobots-examples/data/austin_boundaries\")\n",
    "austin_df = adp.toDf(austin_rdd, spark)\n",
    "austin_df = austin_df.select(austin_df.geometry)\n",
    "austin_df.createOrReplaceTempView(\"austin_df\")\n",
    "austin_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddec6a",
   "metadata": {},
   "source": [
    "### Find Average Predicted Airbnb Price in Each Zip Code Bounday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb409ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = spark.sql(\"select a.geometry as geometry, b.predicted_log_price as pred_price, b.log_price as price from austin_df a left outer join test_df b on ST_Contains(a.geometry, b.airbnb_loc) == True\")\n",
    "joined_df = joined_df.na.fill(value=0,subset=[\"pred_price\"])\n",
    "joined_df = joined_df.na.fill(value=0,subset=[\"price\"])\n",
    "joined_df.createOrReplaceTempView(\"joined_df\")\n",
    "\n",
    "joined_df = spark.sql(\"select geometry, avg(pred_price) as avg_pred_price, avg(price) as avg_price from joined_df group by geometry\")\n",
    "joined_df.createOrReplaceTempView(\"joined_df\")\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b8e728",
   "metadata": {},
   "source": [
    "## Visualize Predicted Average Airbnb Price in Each Zip Code Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf = gpd.GeoDataFrame(joined_df.toPandas(), geometry = \"geometry\", crs = \"EPSG:4326\")\n",
    "joined_gdf.plot(column='avg_pred_price', cmap='OrRd', edgecolor='k', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplot plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
